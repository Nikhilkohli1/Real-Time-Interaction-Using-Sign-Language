{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sign Language Prediction from Video **\n",
    "\n",
    "We are using LSA64 Argentinian Sign Language Video Dataset for this which consist of 3200 Videos of 64 words. We have selected 24 such words for this project which has 1200 videos in total to train our models. We are trying to do a real time conversation with Google Assistant or Alexa using sign languages from these videos as a input. \n",
    "\n",
    "We have used 2 types of models to test this - \n",
    "1. Time Distributed CNN and RNN\n",
    "2. CNN for feature extraction and LSTM\n",
    "\n",
    "Our best accuracy is training 96% and testing 95% using model 2. \n",
    "We first break down the Each video from the dataset into frames, keeping track of the number of frames in each. For our prediction purpose, we selected the sequence length as 40 for each video. We then feed those individual frames into a CNN. For CNN, we used Inception V3 model pre-trained on ImageNet to extract features and save it as a Sequence .npy file. We can now load these extracted features into our LSTM layer which will learn the temporal features from the video and predict the output word for each video. \n",
    "\n",
    "For Final prediction, we load the weights and feed the LSTM with the extracted features through .npy files for any word and create a sentence based on that which is further converted into speech to interact with the Google Assistant. The output of the Google Assistant is then converted to text as well for the end user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D,\n",
    "    MaxPooling2D)\n",
    "from collections import deque\n",
    "import sys\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import time\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import win32com.client as wincl\n",
    "import speech_recognition as sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pipeline():\n",
    "    \n",
    "    weather = ['063_010_002', '062_010_002']\n",
    "    music = ['064_002_002', '053_010_002', '036_010_002']\n",
    "    call = ['064_002_002', '017_002_004']\n",
    "    dance = ['064_002_002', '057_003_001']\n",
    "    argentina = ['028_001_005', '024_002_001']\n",
    "    opaque = ['063_010_002', '001_003_003']\n",
    "    vision = ['033_003_004']\n",
    "    bday = ['060_004_002','050_004_001', '030_006_004']\n",
    "    \n",
    "    pipe = []\n",
    "    pipe.append(weather)\n",
    "    pipe.append(vision) \n",
    "    pipe.append(argentina)\n",
    "    pipe.append(opaque)\n",
    "    pipe.append(bday)\n",
    "    pipe.append(dance)\n",
    "    #pipe.append(call)\n",
    "    pipe.append(music)\n",
    "    \n",
    "    labels = pd.read_csv('data_file.csv')\n",
    "    pred_label = pd.read_csv('Words_Class.csv')\n",
    "    print('Defining the Model and loading Weights..')\n",
    "    seq_len = 40\n",
    "    dim = 2048\n",
    "    input_shape = (seq_len, dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(2048,return_sequences=False,\n",
    "                           input_shape=input_shape,\n",
    "                           dropout=0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(24, activation='softmax'))\n",
    "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                           metrics=metrics)\n",
    "    \n",
    "\n",
    "    model.summary()\n",
    "    model.load_weights('lstm-features.008-0.068.hdf5')\n",
    "    print('Weights loaded')\n",
    "    \n",
    "    for data in pipe:\n",
    "        \n",
    "        sentence = []\n",
    "\n",
    "        for sign in data:\n",
    "            sequence, y = [], []\n",
    "            features = sign + '-40-features.npy'\n",
    "            \n",
    "            X= np.load(features)\n",
    "            sequence.append(X)\n",
    "            X_test = np.array(sequence)\n",
    "            Y_pred = model.predict_classes(X_test)\n",
    "            Y_actual = labels.loc[labels['VideoID'] == sign].Class\n",
    "            Predicted = pred_label.loc[pred_label['Prediction'] == Y_pred[0]].Class\n",
    "            sentence.append(Predicted)\n",
    "\n",
    "        print('Generating Prediction for Signs..')\n",
    "\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            sent.append(word.iloc[0])\n",
    "        final_sent = ' '.join(sent)  \n",
    "\n",
    "        print('Converting to speech.. ')\n",
    "        print(\"Okay Google \" + final_sent)\n",
    "\n",
    "        speak = wincl.Dispatch(\"SAPI.SpVoice\")\n",
    "        r = sr.Recognizer()\n",
    "        speak.Speak(\"Okay Google  \" + final_sent)\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"listening..\")\n",
    "            r.adjust_for_ambient_noise(source)\n",
    "            audio = r.listen(source)\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"Speech to Text: \" + text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the Model and loading Weights..\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 2048)              33562624  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                12312     \n",
      "=================================================================\n",
      "Total params: 34,624,024\n",
      "Trainable params: 34,624,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Weights loaded\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google What is the Weather\n",
      "listening..\n",
      "Speech to Text: now in Boston it's 49 and cloudy today it'll be rainy with a forecasted high of 49 and a low of 48 and tomorrow it'll be cloudy\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google What is Computer Vision\n",
      "listening..\n",
      "Speech to Text: Wikipedia computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google Where is Argentina\n",
      "listening..\n",
      "Speech to Text: is 5600 miles away\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google What is Opaque\n",
      "listening..\n",
      "Speech to Text: Wikipedia opacity is the measure of impenetrability to electromagnetic or other kinds of radiation especially visible light\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google When is Your Birthday\n",
      "listening..\n",
      "Speech to Text: in 2016 but Google's birthday is September 27th so let's say that's my birthday\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google Can you Dance\n",
      "listening..\n",
      "Speech to Text: seems fun\n",
      "Generating Prediction for Signs..\n",
      "Converting to speech.. \n",
      "Okay Google Can you Play some Music\n",
      "listening..\n",
      "Speech to Text: music on Spotify\n"
     ]
    }
   ],
   "source": [
    "pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
